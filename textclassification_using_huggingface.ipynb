{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7_Ot6KbQEdK"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "This notebook demonstrates the process of text classification using three different transformer models: DistilBERT, BERT, and RoBERTa. We will train each model on labeled data, use them to label unlabeled data, and visualize the results.\n",
        "\n",
        "**Steps to perform text-classification**\n",
        "1. Prepare and preprocess data.\n",
        "2. Hyperparameter Tuning using DistilBERT\n",
        "3. Train and evaluate DistilBERT, BERT, and RoBERTa models.\n",
        "4. Label the unlabeled dataset using the trained models.\n",
        "5. Visualize the label distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pR4BEvBQEdL"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SMGB09PV3_u",
        "outputId": "8814f88f-7c48-457e-f320-59f6e50c2b89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting transformers==4.18.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl.metadata (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (2.32.3)\n",
            "Collecting sacremoses (from transformers==4.18.0)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.18.0)\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0) (1.4.2)\n",
            "Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, sacremoses, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sacremoses-0.1.1 tokenizers-0.12.1 transformers-4.18.0\n",
            "Collecting tensorflow==2.16.1\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.16.1)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.64.1)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow==2.16.1)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.1) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.16.1) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.1) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.1) (0.1.2)\n",
            "Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-dtypes, tensorboard, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ml-dtypes-0.3.2 tensorboard-2.16.2 tensorflow-2.16.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.18.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from transformers) (0.1.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers) (1.4.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement emoji2emotion (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for emoji2emotion\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install transformers==4.18.0\n",
        "!pip install tensorflow==2.16.1\n",
        "!pip install transformers torch\n",
        "!pip install emoji2emotion\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GQQCTSvQsQY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification #DistilBERT\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification #BERT\n",
        "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification #RoBERTa\n",
        "\n",
        "from transformers import logging, TFTrainingArguments\n",
        "from transformers.trainer_tf import TFTrainer\n",
        "# from transformers import TextClassificationPipeline\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxApz_Bln3mg"
      },
      "source": [
        "**CODE FOR ADDING EMOJI_EMOTION SECTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi6WgAuuuOJW"
      },
      "outputs": [],
      "source": [
        "!pip install emoji\n",
        "!pip install emoji_emotion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dCRpX5B5oFCG",
        "outputId": "a6fa1d6a-297b-460b-a617-a15503f53d56"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'emoji_emotion'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-439edcdb21e3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0memoji_emotion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmojiEmotion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load your dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/finalHindiDataset_withEmojis.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'emoji_emotion'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from emoji_emotion import EmojiEmotion\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('/content/finalHindiDataset_withEmojis.csv')\n",
        "\n",
        "# Initialize the EmojiEmotion class\n",
        "emoji_emotion = EmojiEmotion()\n",
        "\n",
        "# Function to get emotions from emojis\n",
        "def extract_emotion(emoji_text):\n",
        "    if emoji_text and emoji_text != \"neutral\":\n",
        "        emojis = emoji_text.split()  # Adjust delimiter if needed\n",
        "        emotion_counts = {emotion: 0 for emotion in ['happy', 'sadness', 'anger', 'fear', 'surprise', 'disgust']}\n",
        "\n",
        "        for emoji in emojis:\n",
        "            emotion = emoji_emotion.get(emoji)\n",
        "            if emotion in emotion_counts:\n",
        "                emotion_counts[emotion] += 1\n",
        "\n",
        "        # Determine the emotion with the highest count\n",
        "        detected_emotion = max(emotion_counts, key=emotion_counts.get)\n",
        "        return detected_emotion if emotion_counts[detected_emotion] > 0 else 'none'\n",
        "\n",
        "    return 'neutral'  # If no emoji or it's neutral\n",
        "\n",
        "# Apply the function to the emoji column\n",
        "df['detected_emotion'] = df['Emojis'].apply(extract_emotion)\n",
        "\n",
        "# Save the updated dataset\n",
        "df.to_csv('updated_dataset_with_detected_emotions.csv', index=False)\n",
        "\n",
        "print(\"Emotion detection from emojis complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gra8zfvn7i_Y"
      },
      "source": [
        "**CODE FOR ADDING SENTIMENT ANALYSIS COLUMN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPb8_Pql6ify"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/finalHindiDataset_withEmojis.csv')\n",
        "\n",
        "# Load pre-trained model for Hindi sentiment analysis\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ai4bharat/indic-bert\")\n",
        "\n",
        "# Initialize sentiment analysis pipeline\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, truncation=True)\n",
        "\n",
        "# Function to get sentiment for Hindi text using Hugging Face pipeline\n",
        "def get_hindi_sentiment(text):\n",
        "    # Ensure text is truncated to 512 tokens max\n",
        "    result = sentiment_analysis(text[:512])  # This ensures the text fits within the model's limit\n",
        "    sentiment_label = result[0]['label']\n",
        "    return 'positive' if sentiment_label == 'LABEL_1' else 'negative'\n",
        "\n",
        "# Add sentiment column to the dataframe\n",
        "df['hindi_sentiment'] = df['translated_text'].apply(get_hindi_sentiment)\n",
        "\n",
        "# Save the updated dataset\n",
        "df.to_csv('updated_dataset_with_hindi_sentiment.csv', index=False)\n",
        "\n",
        "print(\"Hindi sentiment analysis column added successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_cEmwG6RDwK"
      },
      "source": [
        "## Data Preprocessing\n",
        "In this section, we will load the dataset, preprocess the data, and split it into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frJ3cafYWXRo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "labeled_df=pd.read_csv('/content/finalHindiDataset_ithEmojis.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wkKGrRN44e4"
      },
      "outputs": [],
      "source": [
        "labeled_df = labeled_df.sample(frac=1).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zx2d22MI4Iva"
      },
      "outputs": [],
      "source": [
        "labeled_df=labeled_df[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLKhXDN5R8h6"
      },
      "outputs": [],
      "source": [
        "# checking count for each label\n",
        "labeled_df['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5OrjnDSPNAj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pGMYz5LPM53"
      },
      "outputs": [],
      "source": [
        "# labeled_df=labeled_df.dropna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnaWSkT-YXsS"
      },
      "outputs": [],
      "source": [
        "labeled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbmbG_EQSJMR"
      },
      "outputs": [],
      "source": [
        "# print(labeled_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaB3gcWzSkQQ"
      },
      "outputs": [],
      "source": [
        "# Preparing data for training and validation\n",
        "data_texts = labeled_df['translated_text'].to_list()\n",
        "data_labels = labeled_df['Label'].to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_98DTHASsRP"
      },
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F_2zmzrSvBM"
      },
      "outputs": [],
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(data_texts, data_labels, test_size = 0.2, random_state = 0 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCW2N7q6QEdP"
      },
      "source": [
        "## Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra4auCMVmJLt"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    '''\n",
        "    function to compute metrics like accuracy, precision, recall, and F1-score to assess model performance\n",
        "    '''\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH6730dUS9BV"
      },
      "source": [
        "### DistilBERT\n",
        "\n",
        "In this section, we will train a DistilBERT model on the labeled data and evaluate its performance.\n",
        "\n",
        "DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than google-bert/bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.\n",
        "\n",
        "**Steps:**\n",
        "1. **Load Tokenizer and Model**: Initialize the tokenizer and model from the Hugging Face library.\n",
        "2. **Tokenize Data**: Tokenize the data using the DistilBERT tokenizer.\n",
        "3. **Create Datasets**: Prepare the training and validation datasets using tokenized data.\n",
        "4. **Define Training Arguments**: Set the parameters for training, such as learning rate and batch size.\n",
        "5. **Train the Model**: Train the DistilBERT model using the training dataset.\n",
        "6. **Evaluate the Model**: Evaluate the trained model on the validation dataset.\n",
        "7. **Save the Model**: Save the trained model and tokenizer for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7x-dvLBUrCB"
      },
      "outputs": [],
      "source": [
        "# pip install --upgrade tensorflow transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHIsaFKhS-8o"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer from the pre-trained DistilBERT model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "# Load the pre-trained DistilBERT model for sequence classification\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WydQZSBbQEdQ"
      },
      "outputs": [],
      "source": [
        "# tokenizing train_texts and val_texts\n",
        "train_encodings = tokenizer(train_texts, truncation = True, padding = True )\n",
        "val_encodings = tokenizer(val_texts, truncation = True, padding = True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRTRqDWUTrR8"
      },
      "outputs": [],
      "source": [
        "# Creating TensorFlow datasets for training and validation\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD2nZekhb01D"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(learning_rate, epochs):\n",
        "    # Define training arguments\n",
        "    training_args = TFTrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=64,\n",
        "        warmup_steps=30,\n",
        "        logging_dir='./logs',\n",
        "        eval_steps=10\n",
        "    )\n",
        "\n",
        "    # Using a distributed training\n",
        "    with training_args.strategy.scope():\n",
        "        trainer_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 2 )\n",
        "\n",
        "    # Initializing the TFTrainer\n",
        "    trainer = TFTrainer(\n",
        "        model=trainer_model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Training the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluating the model\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    return eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdObxvr1QEdQ"
      },
      "source": [
        "#### **Hyperparameter Tuning**\n",
        "\n",
        "In this section, we aim to explore the effect of different hyperparameters on DistilBERT model performance. Specifically, we will experiment with various learning rates and the number of training epochs. The chosen hyperparameters for testing are:\n",
        "\n",
        "- **Learning Rates**: 5e-5, 3e-5, 2e-5\n",
        "- **Epochs**: 5, 6, 7, 8, 9\n",
        "\n",
        "We will train and evaluate the model for each combination of learning rate and epochs. The evaluation metrics we will consider include accuracy, precision, recall, F1 score, and loss. These metrics will help us determine the optimal hyperparameters for our task.\n",
        "\n",
        "The code below performs the hyperparameter testing and stores the results in a DataFrame for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeRe-BmAQEdQ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# hyperparameters to test\n",
        "learning_rates = [5e-5, 6e-5, 9e-5]\n",
        "epochs_list = [5,6,7,8,9]\n",
        "\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for epochs in epochs_list:\n",
        "        print(f\"Training with learning rate: {lr} and epochs: {epochs}\")\n",
        "        eval_results = train_and_evaluate(learning_rate=lr, epochs=epochs)\n",
        "        results.append({\n",
        "            'learning_rate': lr,\n",
        "            'epochs': epochs,\n",
        "            'accuracy': eval_results['eval_accuracy'],\n",
        "            'precision': eval_results['eval_precision'],\n",
        "            'loss': eval_results['eval_loss'],\n",
        "            'recall': eval_results['eval_recall'],\n",
        "            'f1': eval_results['eval_f1']\n",
        "        })\n",
        "\n",
        "# Converting results to DataFrame\n",
        "results_df = pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzFA5laQCPU1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip1MNNDAQEdR"
      },
      "outputs": [],
      "source": [
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwl9IUb4QEdR"
      },
      "source": [
        "#### Results\n",
        "We trained the model with various learning rates and epochs. The following graphs show the performance of the model for different hyperparameter combinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x88UqizyQEdR"
      },
      "outputs": [],
      "source": [
        "results_df.to_csv(\"hyperparameter_tuning_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHxETWGoQEdR"
      },
      "outputs": [],
      "source": [
        "# hyperparameter tuning results\n",
        "\n",
        "# Plot accuracy\n",
        "accuracy_fig = go.Figure()\n",
        "for lr in learning_rates:\n",
        "    subset = results_df[results_df['learning_rate'] == lr]\n",
        "    accuracy_fig.add_trace(go.Scatter(x=subset['epochs'], y=subset['accuracy'], mode='lines', name=f'LR={lr}',\n",
        "                                      hovertemplate='Learning Rate: %{customdata[0]}<br>Epochs: %{x}<br>Accuracy: %{customdata[1]}<br>Loss: %{customdata[2]}',\n",
        "                                       customdata=subset[['learning_rate', 'accuracy', 'loss']]))\n",
        "\n",
        "accuracy_fig.update_layout(\n",
        "    xaxis_title='Epochs',\n",
        "    yaxis_title='Accuracy',\n",
        "    title='Accuracy vs. Epochs for Different Learning Rates',\n",
        "    legend_title='Learning Rate',\n",
        "    template='plotly_white',\n",
        "    width=800\n",
        ")\n",
        "\n",
        "accuracy_fig.show()\n",
        "\n",
        "# Plot F1-score\n",
        "f1_fig = go.Figure()\n",
        "for lr in learning_rates:\n",
        "    subset = results_df[results_df['learning_rate'] == lr]\n",
        "    f1_fig.add_trace(go.Scatter(x=subset['epochs'], y=subset['f1'], mode='lines', name=f'LR={lr}',\n",
        "                                hovertemplate='Learning Rate: %{customdata[0]}<br>Epochs: %{x}<br>F1 Score: %{customdata[1]}<br>Loss: %{customdata[2]}',\n",
        "                                       customdata=subset[['learning_rate', 'f1', 'loss']]))\n",
        "\n",
        "f1_fig.update_layout(\n",
        "    xaxis_title='Epochs',\n",
        "    yaxis_title='F1 Score',\n",
        "    title='F1 Score vs. Epochs for Different Learning Rates',\n",
        "    legend_title='Learning Rate',\n",
        "    template='plotly_white',\n",
        "    width=800\n",
        ")\n",
        "\n",
        "f1_fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZygcIHZQEdR"
      },
      "source": [
        "**Inference drawn:**\n",
        "\n",
        "After conducting hyperparameter tuning by experimenting with different learning rates and epochs, we observe the following results:\n",
        "\n",
        "- **Learning Rate 0.00005:**\n",
        "\n",
        "    - Achieves the highest accuracy of 95.31% with 7 epochs.\n",
        "    - Shows a precision of 94.66%, recall of 94.53%, and F1-score of 94.52%..\n",
        "\n",
        "- **Learning Rate 0.00003:**\n",
        "\n",
        "    - Achieves the highest accuracy of 96.09% with 9 epochs.\n",
        "    - Shows a precision of 96.13%, recall of 96.09%, and F1-score of 96.10%.\n",
        "\n",
        "- **Learning Rate 0.00002:**\n",
        "\n",
        "    - Achieves the highest accuracy of 95.31% with 9 epochs.\n",
        "    - Shows a precision of 95.42%, recall of 95.31%, and F1-score of 95.30%.\n",
        "\n",
        "From these results, the sweet spot appears to be a learning rate of 0.00003 with 9 epochs, as it provides the highest accuracy and well-balanced precision, recall, and F1 scores. This combination offers the best trade-off between training time and model performance, making it the optimal choice for our DistilBERT model on this dataset.\n",
        "\n",
        "**Key Observations**\n",
        "- The model's performance improves as the number of epochs increases, indicating that the model benefits from more training iterations.\n",
        "- Higher learning rates tend to lead to better performance, but the improvements diminish beyond a certain threshold, suggesting the importance of finding the right balance to prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJhx6KBpQEdR"
      },
      "source": [
        "#### Training with optimal parameters\n",
        "\n",
        "Learning rate: 3e-5 and epochs: 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgevfdQuQEdR"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=9,\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=30,\n",
        "    logging_dir='./logs',\n",
        "    eval_steps=10\n",
        ")\n",
        "\n",
        "# Using a distributed training\n",
        "with training_args.strategy.scope():\n",
        "    trainer_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 2 )\n",
        "\n",
        "# Initializing the TFTrainer\n",
        "trainer = TFTrainer(\n",
        "    model=trainer_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6prj1rU9foBA"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADS3HN3_kbQ9"
      },
      "outputs": [],
      "source": [
        "print(\"Learning rate: 3e-5 and epochs: 9\\n\")\n",
        "# Evaluating the model\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSCN6_nJ4-0f"
      },
      "outputs": [],
      "source": [
        "# Save the trained model and tokenizer\n",
        "distilbert_save_directory = \"distilbert_saved_models/\"\n",
        "trainer_model.save_pretrained(distilbert_save_directory)\n",
        "tokenizer.save_pretrained(distilbert_save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k7aMkwFQEdS"
      },
      "source": [
        "### BERT\n",
        "Now, we will train and evaluate a BERT model using the same dataset.\n",
        "\n",
        "BERT is a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction.\n",
        "\n",
        "**Steps:**\n",
        "1. **Load BERT Tokenizer and Model**: Initialize the tokenizer and model from the Hugging Face library.\n",
        "2. **Tokenize Data**: Tokenize the data using the BERT tokenizer.\n",
        "3. **Create Datasets**: Prepare the training and validation datasets using tokenized data.\n",
        "4. **Define Training Arguments**: Set the parameters for training.\n",
        "5. **Train the Model**: Train the BERT model using the training dataset.\n",
        "6. **Evaluate the Model**: Evaluate the performance of the trained BERT model on the validation dataset.\n",
        "7. **Save the Model**: Save the trained model and tokenizer for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBIQagMoQEdS"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer from the pre-trained BERT model\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# Load the pre-trained BERT model for sequence classification\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKU4pK3_QEdS"
      },
      "outputs": [],
      "source": [
        "# tokenizing train_texts and val_texts\n",
        "bert_train_encodings = bert_tokenizer(train_texts, truncation = True, padding = True  )\n",
        "bert_val_encodings = bert_tokenizer(val_texts, truncation = True, padding = True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezqUaEwYQEdS"
      },
      "outputs": [],
      "source": [
        "# Creating TensorFlow datasets for training and validation\n",
        "bert_train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(bert_train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "\n",
        "bert_val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(bert_val_encodings),\n",
        "    val_labels\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfRDKzBqQEdS"
      },
      "source": [
        "#### Training with optimal parameters\n",
        "Learning rate: 3e-5 and epochs: 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zq27eQAVQEdS"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=9,\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8, # reduced batch size to avoid Out Of Memory error\n",
        "    per_device_eval_batch_size=8, # reduced batch size to avoid Out Of Memory error\n",
        "    warmup_steps=30,\n",
        "    logging_dir='./logs',\n",
        "    eval_steps=10\n",
        ")\n",
        "\n",
        "# Using a distributed training\n",
        "with training_args.strategy.scope():\n",
        "    bert_trainer_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2 )\n",
        "\n",
        "# Initializing the TFTrainer\n",
        "trainer = TFTrainer(\n",
        "    model=bert_trainer_model,\n",
        "    args=training_args,\n",
        "    train_dataset=bert_train_dataset,\n",
        "    eval_dataset=bert_val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4sh64mBQEdd"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDOC0XluQEdd"
      },
      "outputs": [],
      "source": [
        "print(\"Model: BERT\")\n",
        "print(\"Learning rate: 3e-5 and epochs: 9\\n\")\n",
        "# Evaluating the model\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KKsbFcVQEdd"
      },
      "outputs": [],
      "source": [
        "# Save the trained model and tokenizer\n",
        "bert_save_directory = \"bert_saved_models/\"\n",
        "bert_trainer_model.save_pretrained(bert_save_directory)\n",
        "bert_tokenizer.save_pretrained(bert_save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXutQQZAQEdd"
      },
      "source": [
        "### RoBERTa\n",
        "Finally, we will train and evaluate a RoBERTa model using the same dataset.\n",
        "\n",
        "RoBERTa builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.\n",
        "\n",
        "**It is same as BERT with better pretraining tricks:**\n",
        "\n",
        "- dynamic masking: tokens are masked differently at each epoch, whereas BERT does it once and for all\n",
        "- train with larger batches\n",
        "- use BPE with bytes as a sub-unit and not characters (because of unicode characters)\n",
        "\n",
        "**Steps:**\n",
        "1. **Load RoBERTa Tokenizer and Model**: Initialize the tokenizer and model from the Hugging Face library.\n",
        "2. **Tokenize Data**: Tokenize the data using the RoBERTa tokenizer.\n",
        "3. **Create Datasets**: Prepare the training and validation datasets using tokenized data.\n",
        "4. **Define Training Arguments**: Set the parameters for training.\n",
        "5. **Train the Model**: Train the RoBERTa model using the training dataset.\n",
        "6. **Evaluate the Model**: Evaluate the performance of the trained RoBERTa model on the validation dataset.\n",
        "7. **Save the Model**: Save the trained model and tokenizer for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBBhbSW5QEdd"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer from the pre-trained RoBERTa model\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# Load the pre-trained RoBERTa model for sequence classification\n",
        "roberta_model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzdK0mTOQEde"
      },
      "outputs": [],
      "source": [
        "# tokenizing train_texts and val_texts\n",
        "roberta_train_encodings = roberta_tokenizer(train_texts, truncation=True, padding=True)\n",
        "roberta_val_encodings = roberta_tokenizer(val_texts, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-RDEFi7QEde"
      },
      "outputs": [],
      "source": [
        "# Creating TensorFlow datasets for training and validation\n",
        "roberta_train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(roberta_train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "\n",
        "roberta_val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(roberta_val_encodings),\n",
        "    val_labels\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLLFkriLQEde"
      },
      "source": [
        "#### Training with optimal parameters\n",
        "Learning rate: 3e-5 and epochs: 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N70UJ0c7QEde"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=9,\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8, # reduced batch size to avoid Out Of Memory error\n",
        "    per_device_eval_batch_size=8, # reduced batch size to avoid Out Of Memory error\n",
        "    warmup_steps=30,\n",
        "    logging_dir='./logs',\n",
        "    eval_steps=10\n",
        ")\n",
        "\n",
        "# Using a distributed training\n",
        "with training_args.strategy.scope():\n",
        "    roberta_trainer_model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels = 3 )\n",
        "\n",
        "# Initializing the TFTrainer\n",
        "trainer = TFTrainer(\n",
        "    model=roberta_trainer_model,\n",
        "    args=training_args,\n",
        "    train_dataset=roberta_train_dataset,\n",
        "    eval_dataset=roberta_val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxjbldGdQEde"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfiQnIi_QEde"
      },
      "outputs": [],
      "source": [
        "print(\"Model: RoBERTa\")\n",
        "print(\"Learning rate: 3e-5 and epochs: 9\\n\")\n",
        "# Evaluating the model\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQTaxhodQEde"
      },
      "outputs": [],
      "source": [
        "# Save the trained model and tokenizer\n",
        "roberta_save_directory = \"roberta_saved_models/\"\n",
        "roberta_trainer_model.save_pretrained(roberta_save_directory)\n",
        "roberta_tokenizer.save_pretrained(roberta_save_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGCIyL9q5giy"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned tokenizer and model from the saved directory\n",
        "tokenizer_fine_tuned = DistilBertTokenizer.from_pretrained(distilbert_save_directory)\n",
        "model_fine_tuned = TFDistilBertForSequenceClassification.from_pretrained(distilbert_save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "571DGfZZQEdg"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Implemented and evaluated 3 pre-trained transformer models—DistilBERT, BERT, and RoBERTa—on a text classification task. The primary goal was to determine the most effective model for classifying the dataset into three categories: FIN_TABLE, NOISE, and TEXT. Through hyperparameter tuning and subsequent evaluations, following conclusions were derived:\n",
        "\n",
        "#### Hyperparameter Tuning\n",
        "\n",
        "Hyperparameter tuning was performed using DistilBERT due to its faster and more lightweight nature. The learning rates and epochs were varied, and the optimal combination was identified based on evaluation metrics such as accuracy, precision, recall, and F1 score.\n",
        "\n",
        "- **Learning Rates:** 5e-5, 3e-5, 2e-5\n",
        "- **Epochs:** 5, 6, 7, 8, 9\n",
        "\n",
        "The optimal hyperparameters identified for DistilBERT were a learning rate of 3e-5 and 9 epochs, resulting in the highest performance across all evaluation metrics.\n",
        "\n",
        "| Learning Rate | Epochs | Accuracy | Precision | Loss     | Recall   | F1       |\n",
        "|---------------|--------|----------|-----------|----------|----------|----------|\n",
        "| 3e-5          | 9      | 0.960938 | 0.961328  | 0.188554 | 0.960938 | 0.960983 |\n",
        "\n",
        "#### Model Evaluation\n",
        "\n",
        "Using the optimal hyperparameters identified, training was performed on all three models:\n",
        "\n",
        "1. **DistilBERT Model**\n",
        "   - **Learning Rate:** 3e-5\n",
        "   - **Epochs:** 9\n",
        "   - **Evaluation Metrics:**\n",
        "     - **Loss:** 0.1885537952184677\n",
        "     - **Accuracy:** 0.9609375\n",
        "     - **Precision:** 0.961328125\n",
        "     - **Recall:** 0.9609375\n",
        "     - **F1 Score:** 0.9609832569391393\n",
        "\n",
        "2. **BERT Model**\n",
        "   - **Learning Rate:** 3e-5\n",
        "   - **Epochs:** 9\n",
        "   - **Evaluation Metrics:**\n",
        "     - **Loss:** 0.33096411228179934\n",
        "     - **Accuracy:** 0.9\n",
        "     - **Precision:** 0.9034313725490195\n",
        "     - **Recall:** 0.9\n",
        "     - **F1 Score:** 0.9003702603702604\n",
        "\n",
        "3. **RoBERTa Model**\n",
        "   - **Learning Rate:** 3e-5\n",
        "   - **Epochs:** 9\n",
        "   - **Evaluation Metrics:**\n",
        "     - **Loss:** 0.3747982978820801\n",
        "     - **Accuracy:** 0.9125\n",
        "     - **Precision:** 0.9145167895167894\n",
        "     - **Recall:** 0.9125\n",
        "     - **F1 Score:** 0.9127909226190475\n",
        "\n",
        "Based on the evaluation metrics, DistilBERT outperformed both BERT and RoBERTa in terms of accuracy, precision, recall, and F1 score. Despite being a lighter and faster model, DistilBERT achieved a higher evaluation performance, making it the most effective model for this text classification task.\n",
        "\n",
        "- **DistilBERT** demonstrated superior performance with an F1 score of 0.96098, making it the best choice for the task.\n",
        "- **BERT** and **RoBERTa**, while still highly effective, did not perform as well as DistilBERT under the same hyperparameters.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KH6730dUS9BV",
        "HdObxvr1QEdQ",
        "Wwl9IUb4QEdR",
        "4k7aMkwFQEdS",
        "SXutQQZAQEdd"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5112486,
          "sourceId": 8554899,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30699,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}