{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBkwgH3qsN2s"
      },
      "source": [
        "STEP-0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CtW-Rk3cQ_e",
        "outputId": "40d4d171-2609-40a3-bf5d-e18f803a1e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cpu)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.2)\n",
            "Collecting sphinx>=5.1.0 (from sphinx-argparse->indic-nlp-library)\n",
            "  Downloading sphinx-8.1.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting docutils>=0.19 (from sphinx-argparse->indic-nlp-library)\n",
            "  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
            "Collecting sphinxcontrib-applehelp>=1.0.7 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting sphinxcontrib-devhelp>=1.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting sphinxcontrib-htmlhelp>=2.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting sphinxcontrib-jsmath>=1.0.1 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting sphinxcontrib-qthelp>=1.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.4)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
            "Collecting snowballstemmer>=2.2 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
            "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting babel>=2.13 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
            "  Downloading babel-2.16.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting alabaster>=0.7.14 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
            "  Downloading alabaster-1.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting imagesize>=1.3 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
            "  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.2)\n",
            "Requirement already satisfied: tomli>=2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2024.8.30)\n",
            "Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docutils-0.21.2-py3-none-any.whl (587 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.4/587.4 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx-8.1.3-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alabaster-1.0.0-py3-none-any.whl (13 kB)\n",
            "Downloading babel-2.16.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.3/119.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
            "Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: snowballstemmer, morfessor, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, imagesize, docutils, babel, alabaster, sphinx, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed alabaster-1.0.0 babel-2.16.0 docutils-0.21.2 imagesize-1.4.1 indic-nlp-library-0.92 morfessor-2.0.6 snowballstemmer-2.2.0 sphinx-8.1.3 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas scikit-learn numpy nltk gensim transformers torch\n",
        "!pip install indic-nlp-library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A12kRA46sZz9"
      },
      "source": [
        "STEP-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhbXjB1MsbKG",
        "outputId": "dea26af1-19a8-4c08-81d0-93cc677b1a01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9907 entries, 0 to 9906\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   Unnamed: 0       9907 non-null   int64  \n",
            " 1   Label            9899 non-null   float64\n",
            " 2   Aug_text         9907 non-null   object \n",
            " 3   translated_text  9907 non-null   object \n",
            " 4   word_count       9907 non-null   int64  \n",
            " 5   Emojis           9907 non-null   object \n",
            " 6   Unnamed: 6       0 non-null      float64\n",
            " 7   Unnamed: 7       0 non-null      float64\n",
            " 8   Unnamed: 8       0 non-null      float64\n",
            "dtypes: float64(4), int64(2), object(3)\n",
            "memory usage: 696.7+ KB\n",
            "None\n",
            "   Unnamed: 0  Label                                           Aug_text  \\\n",
            "0           0    0.0  3. A day of historic importance and national j...   \n",
            "1           1    0.0  4. A momentous occasion for India - Prime Mini...   \n",
            "2           2    0.0   nobody else has taken over as the Home Minist...   \n",
            "3           3    0.0  I am taking over as the Home Minister of India...   \n",
            "4           4    0.0  I have assumed the role of Home Minister of In...   \n",
            "\n",
            "                                     translated_text  word_count   Emojis  \\\n",
            "0  भारत के लिए ऐतिहासिक महत्व और राष्ट्रीय आनंद क...          20  neutral   \n",
            "1  भारत के लिए एक महत्वपूर्ण अवसर - भगवान राम के ...          16  neutral   \n",
            "2  आज भारत के गृह मंत्री के रूप में किसी और ने पद...          13  neutral   \n",
            "3  मैं आज भारत के गृह मंत्री के रूप में पदभार ग्र...          11  neutral   \n",
            "4    मैंने आज भारत के गृह मंत्री की भूमिका निभाई है।          11  neutral   \n",
            "\n",
            "   Unnamed: 6  Unnamed: 7  Unnamed: 8  \n",
            "0         NaN         NaN         NaN  \n",
            "1         NaN         NaN         NaN  \n",
            "2         NaN         NaN         NaN  \n",
            "3         NaN         NaN         NaN  \n",
            "4         NaN         NaN         NaN  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/FinalHindiDataset_withEmojis.csv\"  # Replace with your file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display a summary of the data\n",
        "print(data.info())\n",
        "print(data.head())\n",
        "\n",
        "data.sample(n=100);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCEgSYrxs_PQ"
      },
      "source": [
        "STEP-2: performing LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtLI1Ka7tAqH"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Preprocess the English text\n",
        "english_text = data['Aug_text'].fillna(\"\").tolist()\n",
        "\n",
        "# Convert text to a document-term matrix\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "dtm = vectorizer.fit_transform(english_text)\n",
        "\n",
        "# Fit the LDA model\n",
        "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
        "lda_features = lda.fit_transform(dtm)\n",
        "\n",
        "# Add LDA features to the dataframe\n",
        "lda_df = pd.DataFrame(lda_features, columns=[f\"LDA_Topic_{i}\" for i in range(10)])\n",
        "data = pd.concat([data, lda_df], axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iojfargmS2Dx"
      },
      "source": [
        "NER: named entity relationship"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAPrK3tTwMs9",
        "outputId": "760b95f6-8ad4-4bdd-9ccc-708ec6ab25ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stanza in /usr/local/lib/python3.10/dist-packages (1.9.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from stanza) (2.14.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (4.25.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.4.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.5.1+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.6)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from stanza) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHHsI_IRS6Zo"
      },
      "outputs": [],
      "source": [
        "# # !pip install stanza\n",
        "# import stanza\n",
        "# stanza.download('hi')  # Download the Hindi model\n",
        "\n",
        "# # Initialize Stanza pipeline for Hindi\n",
        "# nlp = stanza.Pipeline('hi')\n",
        "\n",
        "# # Function to extract named entities\n",
        "# def extract_named_entities_stanza(text):\n",
        "#     doc = nlp(text)\n",
        "#     entities = [ent.text for sent in doc.sentences for ent in sent.ents]\n",
        "#     return len(entities)  # Count the named entities\n",
        "\n",
        "# # Apply NER on Hindi text\n",
        "# data['NER_Count'] = data['translated_text'].fillna(\"\").apply(extract_named_entities_stanza)\n",
        "# print(data.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kePcoVStXIH"
      },
      "source": [
        "STEP-3:Fact-checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7smRuVW40mW8"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Load the LIAR dataset (train.tsv)\n",
        "# liar_df = pd.read_csv('/content/liar_train.tsv', sep='\\t', header=None)\n",
        "\n",
        "# # Check the first few rows to understand its structure\n",
        "# print(liar_df.head())\n",
        "\n",
        "# # The columns might not have headers, so we will manually assign them\n",
        "# liar_df.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'speaker_job', 'stateinfo', 'party', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'true_counts', 'class']\n",
        "\n",
        "# # Now extract only the 'statement' column for fact-checking\n",
        "# fact_check_corpus = liar_df['statement'].dropna().tolist()\n",
        "\n",
        "# # Check the first few statements\n",
        "# print(fact_check_corpus[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlBuWult3tFq"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoTokenizer, AutoModel\n",
        "# import torch\n",
        "\n",
        "# # Load the pre-trained tokenizer and model\n",
        "# model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# # Function to ensure proper tokenization and embedding extraction\n",
        "# def get_embeddings(text):\n",
        "#     # Tokenize with truncation and padding\n",
        "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "\n",
        "#     # Forward pass through the model to get token embeddings\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(**inputs)\n",
        "\n",
        "#     # Extract the embeddings for the [CLS] token (first token) or average pooling of token embeddings\n",
        "#     # CLS token embedding: outputs[0] is of shape (batch_size, sequence_length, hidden_size)\n",
        "#     embeddings = outputs.last_hidden_state[0][0]  # [CLS] token embedding (size: hidden_size)\n",
        "\n",
        "#     # Alternatively, if you want to average over all token embeddings:\n",
        "#     # embeddings = outputs.last_hidden_state[0].mean(dim=0)\n",
        "\n",
        "#     return embeddings.numpy()  # Convert to numpy array\n",
        "\n",
        "# # Example usage for fact-check corpus\n",
        "# fact_check_corpus = liar_df['statement'].dropna().tolist()\n",
        "\n",
        "# # Convert LIAR fact-check claims into embeddings\n",
        "# fact_embeddings = [get_embeddings(text) for text in fact_check_corpus]\n",
        "\n",
        "# # Check the shape of the first embedding to ensure it's consistent\n",
        "# print(fact_embeddings[0].shape)  # Should print (512,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtdsWYbquS54"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# # Compute alignment score for each English text\n",
        "# english_text = data['Aug_text'].fillna(\"\").tolist()\n",
        "\n",
        "# fact_alignment_scores = []\n",
        "\n",
        "# for text in english_text:\n",
        "#     text_embedding = get_embeddings(text)\n",
        "\n",
        "#     # Compute the cosine similarity for each fact-check claim\n",
        "#     scores = [cosine_similarity([text_embedding], [fact])[0][0] for fact in fact_embeddings]\n",
        "#     fact_alignment_scores.append(max(scores))  # Get the highest alignment score\n",
        "\n",
        "# # Add the fact alignment scores to your dataset\n",
        "# data['Fact_Alignment_Score'] = fact_alignment_scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yB66MavukH6"
      },
      "source": [
        "STEP-4: INDIC bert embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54XMgMWAunGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2491131b-80f1-43ba-d311-ab6a07207c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "\n",
        "# Load Indic-BERT model and tokenizer\n",
        "model_name = \"ai4bharat/indic-bert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Function to get embeddings\n",
        "def get_embeddings(text):\n",
        "    # Add max_length and truncation to tokenizer to handle long sequences\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "# Extract embeddings for Hindi text\n",
        "hindi_embeddings = [get_embeddings(text) for text in data['translated_text'].fillna(\"\")]\n",
        "hindi_embeddings = np.vstack(hindi_embeddings)\n",
        "\n",
        "# Add embeddings to the dataframe\n",
        "for i in range(hindi_embeddings.shape[1]):\n",
        "    data[f'Hindi_Embedding_{i}'] = hindi_embeddings[:, i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PtketezK7n3"
      },
      "source": [
        "sentiment polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9S6xs4VK-v9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\"  # or any other sentiment model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Function to analyze sentiment\n",
        "def get_sentiment_hindi(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    outputs = model(**inputs)\n",
        "    scores = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    # Assuming the labels are: 0 = Negative, 1 = Neutral, 2 = Positive\n",
        "    sentiment = torch.argmax(scores, dim=1).item()\n",
        "    return sentiment\n",
        "\n",
        "# Apply sentiment analysis\n",
        "data['Sentiment_Label'] = data['translated_text'].fillna(\"\").apply(get_sentiment_hindi)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnEdK6puv3py"
      },
      "source": [
        "STEP-5:combine all features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODKpgwrHv9JN"
      },
      "outputs": [],
      "source": [
        "# # Select relevant features\n",
        "# # print(data.head());\n",
        "# feature_columns = [col for col in data.columns if col.startswith('LDA_Topic') or\n",
        "#                    col.startswith('Hindi_Embedding') or col in ['NER_Count', 'Sentiment_Label']]\n",
        "\n",
        "# X = data[feature_columns]\n",
        "# y = data['Label']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhObR4kTwCYY"
      },
      "source": [
        "STEP-6: split data for testing and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOxJTV2OwB3A"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # Drop rows with NaN values in the target variable 'Label'\n",
        "# data = data.dropna(subset=['Label'])\n",
        "\n",
        "# # Split the dataset after removing NaN values\n",
        "# X_train, X_test, y_train, y_test = train_test_split(data[feature_columns], data['Label'], test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB_CnLESwHst"
      },
      "source": [
        "STEP-7: train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GQoibtAwNr_"
      },
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# # Define and train the Random Forest classifier\n",
        "# clf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
        "# clf.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvuTDKpewP7p"
      },
      "source": [
        "STEP-8: evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FS22_j59wWt1"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# # Predict on test data\n",
        "# y_pred = clf.predict(X_test)\n",
        "\n",
        "# # Evaluate the model\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# precision = precision_score(y_test, y_pred)\n",
        "# recall = recall_score(y_test, y_pred)\n",
        "# f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# print(f\"Accuracy: {accuracy:.2f}\")\n",
        "# print(f\"Precision: {precision:.2f}\")\n",
        "# print(f\"Recall: {recall:.2f}\")\n",
        "# print(f\"F1-Score: {f1:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgNfU96hEDa2"
      },
      "source": [
        "support vector machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wLTVYtCfp6l"
      },
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, classification_report\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load your dataset (assume `data` is preprocessed)\n",
        "# Replace `feature_columns` and `Label` with actual column names\n",
        "feature_columns = [col for col in data.columns if col.startswith('LDA_Topic') or\n",
        "                   col.startswith('Hindi_Embedding') or col in ['NER_Count', 'Sentiment_Label']]\n",
        "X = data[feature_columns]\n",
        "y = data['Label']\n",
        "data = data.dropna(subset=['Label'])\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Scale the features for better performance of SVM\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 1: Hyperparameter Tuning using GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1]\n",
        "}\n",
        "grid_search = GridSearchCV(SVC(random_state=42), param_grid, cv=5, scoring='accuracy', verbose=1)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best Parameters and Best Score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
        "\n",
        "# Step 2: Cross-Validation Scores\n",
        "cv_scores = cross_val_score(grid_search.best_estimator_, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "print(\"Mean CV Accuracy:\", np.mean(cv_scores))\n",
        "\n",
        "# Step 3: Evaluate Model on Test Data\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "y_pred_proba = best_model.decision_function(X_test_scaled)  # For ROC-AUC\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
        "\n",
        "# Step 4: ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Interpretability with SHAP\n",
        "explainer = shap.Explainer(best_model, X_train_scaled)\n",
        "shap_values = explainer(X_test_scaled)\n",
        "\n",
        "# Summary Plot\n",
        "shap.summary_plot(shap_values, X_test, feature_names=feature_columns)\n",
        "\n",
        "# Individual Force Plot for Example Instance\n",
        "sample_idx = 0  # Change index for other samples\n",
        "shap.force_plot(explainer.expected_value[1], shap_values[sample_idx].values, X_test.iloc[sample_idx],\n",
        "                matplotlib=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fecaSbEjEAxa"
      },
      "source": [
        "Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRqxFht1D1k7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Step 1: Prepare the Data\n",
        "# Assume the data contains required columns: 'LDA_Topic_*', 'Hindi_Embedding_*', 'NER_Count', 'Sentiment_Label', 'Label'\n",
        "\n",
        "# Select feature columns (replace with your actual column names)\n",
        "lda_features = data[[col for col in data.columns if col.startswith('LDA_Topic')]].values\n",
        "hindi_embeddings = data[[col for col in data.columns if col.startswith('Hindi_Embedding')]].values\n",
        "ner_counts = data['NER_Count'].values.reshape(-1, 1)  # Reshape for single-column feature\n",
        "sentiment_labels = data['Sentiment_Label'].values.reshape(-1, 1)  # Reshape for single-column feature\n",
        "\n",
        "# Combine features using NumPy\n",
        "combined_features = np.hstack([lda_features, hindi_embeddings, sentiment_labels])\n",
        "\n",
        "# Extract target labels\n",
        "labels = data['Label'].values\n",
        "\n",
        "# Step 2: Split the Dataset\n",
        "# Drop NaN labels to avoid errors\n",
        "data = data.dropna(subset=['Label'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    combined_features, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Step 3: Train the Random Forest Model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Evaluate the Model\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9e8ilWhFNVc"
      },
      "source": [
        "indic bert at final stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo4sMLByFLIk"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoTokenizer, BertForSequenceClassification\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "# import torch\n",
        "# from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# # Step 1: Load IndicBERT\n",
        "# model_name = \"ai4bharat/indic-bert\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Binary classification\n",
        "\n",
        "# # Step 2: Define a Custom Dataset\n",
        "# class SimpleRumorDataset(Dataset):\n",
        "#     def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "#         self.texts = texts\n",
        "#         self.labels = labels\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_length = max_length\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.texts)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         # Tokenize text\n",
        "#         tokens = self.tokenizer(\n",
        "#             self.texts[idx],\n",
        "#             padding=\"max_length\",\n",
        "#             truncation=True,\n",
        "#             max_length=self.max_length,\n",
        "#             return_tensors=\"pt\"\n",
        "#         )\n",
        "#         label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "#         return {\n",
        "#             \"input_ids\": tokens[\"input_ids\"].squeeze(),\n",
        "#             \"attention_mask\": tokens[\"attention_mask\"].squeeze(),\n",
        "#             \"label\": label\n",
        "#         }\n",
        "\n",
        "# # Step 3: Prepare Dataset\n",
        "# texts = data[\"translated_text\"].tolist()  # Hindi text\n",
        "# labels = data[\"Label\"].values\n",
        "\n",
        "# # Split data\n",
        "# X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
        "#     texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "# )\n",
        "\n",
        "# # Create datasets and dataloaders\n",
        "# train_dataset = SimpleRumorDataset(X_train_texts, y_train, tokenizer)\n",
        "# test_dataset = SimpleRumorDataset(X_test_texts, y_test, tokenizer)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# # Step 4: Define Training Loop\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
        "#         labels = batch[\"label\"].to(device)\n",
        "\n",
        "#         # Forward pass\n",
        "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#         loss = outputs.loss\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         # Backward pass\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#     return total_loss / len(train_loader)\n",
        "\n",
        "# def evaluate(model, test_loader, device):\n",
        "#     model.eval()\n",
        "#     preds = []\n",
        "#     true_labels = []\n",
        "#     with torch.no_grad():\n",
        "#         for batch in test_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "#             outputs = model(input_ids, attention_mask=attention_mask)\n",
        "#             logits = outputs.logits\n",
        "#             predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "#             preds.extend(predictions.cpu().numpy())\n",
        "#             true_labels.extend(batch[\"label\"].cpu().numpy())\n",
        "#     return accuracy_score(true_labels, preds), classification_report(true_labels, preds)\n",
        "\n",
        "# # Step 5: Train and Evaluate\n",
        "# for epoch in range(3):  # Train for 3 epochs\n",
        "#     train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "#     print(f\"Epoch {epoch+1}, Loss: {train_loss:.4f}\")\n",
        "\n",
        "# accuracy, report = evaluate(model, test_loader, device)\n",
        "# print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "# print(\"\\nClassification Report:\\n\", report)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}